{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from typing import Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    CHAR_DICT = dict(zip([char for char in string.ascii_lowercase], range(26)))\n",
    "    CHAR_DICT[' '] = 26\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data(path: str, **kwargs) -> pd.DataFrame:\n",
    "        return pd.read_csv(path, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_word(word: str) -> list:\n",
    "        return [char for char in word]\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_sentence(sentence: str) -> str:\n",
    "        sentence = sentence.lower()\n",
    "        match = re.compile('[^a-z\\s]')\n",
    "        return match.sub('', sentence)\n",
    "\n",
    "    @classmethod\n",
    "    def clean_external_data(cls, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        data2 = data[data['text'] != '[START]'].copy()\n",
    "        data2['text'] = data2['text'].apply(lambda x: cls.clean_sentence(x))\n",
    "        return data2.reset_index()\n",
    "\n",
    "    @classmethod\n",
    "    def make_conversation_pair(cls, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        external_chat_df = pd.DataFrame({\"human\": [], \"robot\": []})\n",
    "        human_talks = \"\"\n",
    "        robot_talks = \"\"\n",
    "\n",
    "        for i in list(range(data.shape[0])):\n",
    "            if data.source[i] == \"human\":\n",
    "                human_talks = human_talks + data.text[i]\n",
    "            else:\n",
    "                robot_talks = robot_talks + data.text[i]\n",
    "\n",
    "            if human_talks != \"\" and robot_talks != \"\":\n",
    "                #                     print(\"Human talks: \" + human_talks)\n",
    "                #                     print(\"Robot talks: \" + robot_talks)\n",
    "                temp_df = pd.DataFrame({\"human\": [human_talks], \"robot\": [robot_talks]})\n",
    "                external_chat_df = external_chat_df.append(temp_df)\n",
    "\n",
    "                human_talks = \"\"\n",
    "                robot_talks = \"\"\n",
    "\n",
    "        return external_chat_df\n",
    "\n",
    "\n",
    "def process_data(local_data_path: str, external_data_path: str, path_to_save: str) -> pd.DataFrame:\n",
    "    print(\"Data processing is going on...\")\n",
    "\n",
    "    # Loading data from all sources\n",
    "    local_data = Utils.get_data(local_data_path)\n",
    "    external_data = Utils.get_data(external_data_path)\n",
    "\n",
    "    local_data['CLIENT'] = local_data['CLIENT'].apply(lambda x: Utils.clean_sentence(x))\n",
    "    local_data['CLIENT'] = local_data['CLIENT'].apply(lambda x: list(x))\n",
    "\n",
    "    external_data['source'] = external_data['source'].apply(lambda x: Utils.clean_sentence(x))\n",
    "\n",
    "    cleaned_external_data = Utils.clean_external_data(external_data)\n",
    "\n",
    "    cleaned_external_data_pair = Utils.make_conversation_pair(cleaned_external_data)\n",
    "    external_chat_df_filtered = cleaned_external_data_pair[cleaned_external_data_pair['human'].map(len) <= 30]\n",
    "\n",
    "    external_chat_df_filtered = external_chat_df_filtered.rename(index=str,\n",
    "                                                                 columns={\"human\": \"CLIENT\", \"robot\": 'ACTIVITY'})\n",
    "\n",
    "    external_chat_df_filtered['CLIENT'] = external_chat_df_filtered['CLIENT'].apply(lambda x: x.lower())\n",
    "    external_chat_df_filtered['CLIENT'] = external_chat_df_filtered['CLIENT'].apply(lambda x: list(x))\n",
    "\n",
    "    all_df = local_data.append(external_chat_df_filtered).reset_index()\n",
    "    all_df['CLIENT'] = all_df['CLIENT'].apply(lambda x: [Utils.CHAR_DICT[i] for i in x])\n",
    "\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = len(Utils.CHAR_DICT.keys())\n",
    "\n",
    "\n",
    "def train_and_evaluate(data: pd.DataFrame, args: Dict):\n",
    "    x = pad_sequences(data['CLIENT'], maxlen=args['sequence_length'])\n",
    "    print('Shape of data tensor:', x.shape)\n",
    "\n",
    "    y = pd.get_dummies(data['ACTIVITY']).values\n",
    "    print('Shape of label tensor:', y.shape)\n",
    "\n",
    "    activity_dict = dict(zip(list(pd.get_dummies(data['ACTIVITY']).columns), range(len(list(pd.get_dummies(data['ACTIVITY']).columns)))))\n",
    "\n",
    "    with open('resources/activities2dummies.json', 'w') as fp:\n",
    "        json.dump(activity_dict, fp)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=args['train_test_ratio'], random_state=42)\n",
    "    print(x_train.shape, y_train.shape)\n",
    "    print(x_test.shape, y_test.shape)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NB_WORDS, args['embedding_size'], input_length=x.shape[1]))\n",
    "    model.add(SpatialDropout1D(args['keep_prob']))\n",
    "    model.add(LSTM(args['sequence_length'], dropout=args['keep_prob'], recurrent_dropout=args['keep_prob']))\n",
    "    model.add(Dense(len(data['ACTIVITY'].unique()), activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        epochs=args['train_steps'], batch_size=args['batch_size'],\n",
    "                        validation_split=args['validation_split'],\n",
    "                    )\n",
    "\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(args['base_dir'], args['output_dir'], \"loss\"))\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(history.history['acc'], label='train')\n",
    "    plt.plot(history.history['val_acc'], label='test')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(args['base_dir'], args['output_dir'], 'accuracy'))\n",
    "\n",
    "    model.save(os.path.join(args['base_dir'], args['output_dir'], 'lstm_model.h5'))  # creates a HDF5 file 'my_model.h5'\n",
    "\n",
    "    print(\"Model training completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --base_dir BASE_DIR --data_dir DATA_DIR\n",
      "                             --local_file_name LOCAL_FILE_NAME\n",
      "                             --external_file_name EXTERNAL_FILE_NAME\n",
      "                             --resources_dir RESOURCES_DIR --output_dir\n",
      "                             OUTPUT_DIR [--embedding_size EMBEDDING_SIZE]\n",
      "                             [--sequence_length SEQUENCE_LENGTH]\n",
      "                             [--num_layers NUM_LAYERS]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--batch_size BATCH_SIZE] [--keep_prob KEEP_PROB]\n",
      "                             [--train_steps TRAIN_STEPS]\n",
      "                             [--train_test_ratio TRAIN_TEST_RATIO]\n",
      "                             [--validation_split VALIDATION_SPLIT]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --base_dir, --data_dir, --local_file_name, --external_file_name, --resources_dir, --output_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumyadip/.local/share/virtualenvs/intent-activity-classifier-ZnxBbNxu/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Directory paths\n",
    "    parser.add_argument('--base_dir',\n",
    "                        help='Root directory',\n",
    "                        required=True\n",
    "                        )\n",
    "    parser.add_argument('--data_dir',\n",
    "                        help='Relative path of data directory',\n",
    "                        required=True\n",
    "                        )\n",
    "    parser.add_argument('--local_file_name',\n",
    "                        help='train file name',\n",
    "                        required=True\n",
    "                        )\n",
    "    parser.add_argument('--external_file_name',\n",
    "                        help='validation file name',\n",
    "                        required=True\n",
    "                        )\n",
    "    parser.add_argument('--resources_dir',\n",
    "                        help='Resourced directory path',\n",
    "                        required=True\n",
    "                        )\n",
    "    parser.add_argument(\n",
    "                        '--output_dir',\n",
    "                        help='Directory to write checkpoints and export models',\n",
    "                        required=True\n",
    "                    )\n",
    "\n",
    "    # Embeddings\n",
    "    parser.add_argument(\"--embedding_size\",\n",
    "                        type=int,\n",
    "                        default=32,\n",
    "                        help=\"Word embedding size. (For glove, use 50 | 100 | 200 | 300)\"\n",
    "                        )\n",
    "\n",
    "    # Model structure\n",
    "    parser.add_argument(\"--sequence_length\",\n",
    "                        type=int,\n",
    "                        default=30,\n",
    "                        help=\"LSTM network length\"\n",
    "                        )\n",
    "    parser.add_argument(\"--num_layers\",\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help=\"LSTM network depth\"\n",
    "                        )\n",
    "\n",
    "    # Train params\n",
    "    parser.add_argument(\"--learning_rate\",\n",
    "                        type=float,\n",
    "                        default=1e-2,\n",
    "                        help=\"Learning rate.\"\n",
    "                    )\n",
    "    parser.add_argument(\"--batch_size\",\n",
    "                        type=int,\n",
    "                        default=16,\n",
    "                        help=\"Batch size.\"\n",
    "                    )\n",
    "    parser.add_argument(\"--keep_prob\",\n",
    "                        type=float,\n",
    "                        default=1,\n",
    "                        help=\"Dropout keep prob.\"\n",
    "                    )\n",
    "    parser.add_argument(\n",
    "                        '--train_steps',\n",
    "                        help='Steps to run the training job for',\n",
    "                        type=int,\n",
    "                        default=300\n",
    "                    )\n",
    "    parser.add_argument(\n",
    "                        '--train_test_ratio',\n",
    "                        help='Train and test data splitting ratio',\n",
    "                        default=1,\n",
    "                        type=float\n",
    "                    )\n",
    "    parser.add_argument(\n",
    "                        '--validation_split',\n",
    "                        help='Validation done on percentage of train data ',\n",
    "                        default=0.1,\n",
    "                        type=float\n",
    "                    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    print(arguments)\n",
    "\n",
    "    local_file_path = os.path.join(arguments['base_dir'], os.path.join(arguments['data_dir'], os.path.join(arguments['local_file_name'])))\n",
    "    external_file_path = os.path.join(arguments['base_dir'], os.path.join(arguments['data_dir'], os.path.join(arguments['external_file_name'])))\n",
    "    resources_dir = os.path.join(arguments['base_dir'], arguments['resources_dir'])\n",
    "\n",
    "    df = process_data(local_file_path, external_file_path, resources_dir)\n",
    "    train_and_evaluate(data=df, args=arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resources/activities2dummies.json', 'r') as fp:\n",
    "    activities_dict = json.load(fp)\n",
    "\n",
    "reverse_dict = {v: k for k,v in activities_dict.items()}\n",
    "\n",
    "model = load_model('/home/soumyadip/Desktop/PycharmProjects/intent-activity-classifier/output/lstm_model.h5')\n",
    "while True:\n",
    "    user_input = input(\"You:  \")\n",
    "    user_input = Utils.clean_sentence(user_input)\n",
    "    user_input = list(user_input)\n",
    "    user_input = [Utils.CHAR_DICT[i] for i in user_input]\n",
    "    y_prob = model.predict(pad_sequences(np.array([user_input]), maxlen=30))\n",
    "    y_class = y_prob.argmax(axis=-1)\n",
    "    print(\"Bot:  \" + reverse_dict[y_class[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
